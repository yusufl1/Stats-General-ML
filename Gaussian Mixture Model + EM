import numpy as np
from matplotlib import pyplot as plt

def normal_density(mu, Sigma, X):
    return np.linalg.det(Sigma) ** -.5 ** (2 * np.pi) ** (-X.shape[1]/2.) \
                * np.exp(-.5 * np.einsum('ij, ij -> i',\
                        X - mu, np.dot(np.linalg.inv(Sigma) , (X - mu).T).T ) )

def log_likelihood(R):
    return np.sum(np.log(np.sum(R, axis = 1)))

def em_e_step(R, w, mu, Sigma, log_likelihoods, X):
    for k in range(2):
        R[:, k] = w[k] * normal_density(mu[k], Sigma[k], X)
    log_lik = log_likelihood(R)
    log_likelihoods.append(log_lik)
    R = (R.T / np.sum(R, axis = 1)).T
    N_ks = np.sum(R, axis = 0)
    return log_likelihoods, R, N_ks

def em_m_step(R, N_ks, X, mu, Sigma, w):
    for k in range(2):
        mu[k] = 1. / N_ks[k] * np.sum(R[:, k] * X.T, axis = 1).T
        x_mu = np.matrix(X - mu[k])
        Sigma[k] = np.array(1 / N_ks[k] * np.dot(np.multiply(x_mu.T,  R[:, k]), x_mu))
        w[k] = 1. / X.shape[0] * N_ks[k]
    return mu, Sigma, w

class GMM:
    def __init__(self, k = 2, eps = 1e-8):
        self.k = k
        self.eps = eps
        from collections import namedtuple

    def fit_EM(self, X, max_iters = 1000):
        n, d = X.shape
        mu = np.array([[0.0, 0.0], [1.0, 1.0]])
        Sigma= [np.eye(d)] * self.k
        w = [1./self.k] * self.k
        R = np.zeros((n, self.k))
        log_likelihoods = []
        while len(log_likelihoods) < max_iters:
            # E - Step
            log_likelihoods, R, N_ks = em_e_step(R, w, mu, Sigma, log_likelihoods, X)
            # M Step
            mu, Sigma, w = em_m_step(R, N_ks, X, mu, Sigma, w)
            if len(log_likelihoods) < 2 : continue
            if np.abs(log_likelihoods[-1] - log_likelihoods[-2]) < self.eps: break
        ## bind all results together
        from collections import namedtuple
        self.params = namedtuple('params', ['mu', 'Sigma', 'w', 'log_likelihoods', 'num_iters'])
        self.params.mu = mu
        self.params.Sigma = Sigma
        self.params.w = w
        self.params.log_likelihoods = log_likelihoods
        self.params.num_iters = len(log_likelihoods)
        return self.params

    def plot_log_likelihood(self):
        import pylab as plt
        plt.plot(self.params.log_likelihoods)
        plt.title('Log Likelihood vs iterations')
        plt.xlabel('Iterations')
        plt.ylabel('log likelihood')
        plt.show()


def predict(params, x):
    p = lambda mu, s : np.linalg.det(s) ** - 0.5 * (2 * np.pi) **\
                (-len(x)/2) * np.exp( -0.5 * np.dot(x - mu , \
                        np.dot(np.linalg.inv(s) , x - mu)))
    probs = np.array([w * p(mu, s) for mu, s, w in \
        zip(params.mu, params.Sigma, params.w)])
    return probs/np.sum(probs)

def assign_to_clusers(params, X):
    cluster1 = []
    cluster2 = []
    for i in range(X.shape[0]):
        x = X[i, :]
        prediction = predict(params, x)
        if prediction[0] > prediction[1]:
            cluster1.append(x)
        else:
            cluster2.append(x)
    return np.array(cluster1), np.array(cluster2)

def check_acc(cluster1_pred, cluster2_pred, cluster1, cluster2, X):
    count = 0
    for i in range(X.shape[0]):
        x_p, y_p = X[i, 0], X[i, 1]
        if ([x_p, y_p] in cluster1.tolist() and [x_p, y_p] in cluster1_pred.tolist()) or \
            ([x_p, y_p] in cluster2.tolist() and [x_p, y_p] in cluster2_pred.tolist()):
            count += 1
    return count/X.shape[0]

if __name__ == "__main__":
    mu1 = np.array([0.1, 0.1])
    mu2 = np.array([6.0, 0.1])
    sigma = np.matrix([[10, 7], [7, 10]])
    sample1 = np.random.multivariate_normal(mu1, sigma, size=200)
    sample2 = np.random.multivariate_normal(mu2, sigma, size=200)
    X = np.concatenate((sample1, sample2))
    GMM = GMM()
    params = GMM.fit_EM(X)
    cluster1_pred, cluster2_pred = assign_to_clusers(params, X)
    accuracy = check_acc(cluster1_pred, cluster2_pred, sample1, sample2, X)
    print(accuracy)
    plt.scatter(cluster1_pred[:,0], cluster1_pred[:,1], c="red")
    plt.scatter(cluster2_pred[:,0], cluster2_pred[:,1], c="blue", marker='x')
    plt.show()
    GMM.plot_log_likelihood()
