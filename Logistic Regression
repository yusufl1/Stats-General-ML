from loadMNIST import *
#import numpy as np
from autograd import numpy as np
from scipy.special import logsumexp
import autograd as ag
from mxnet import nd

N_data, train_images, train_labels, test_images, test_labels = load_mnist()

def softmax(y_linear):
    exp = np.exp(y_linear-np.max(y_linear, axis=1).reshape((-1,1)))
    norms = np.sum(exp, axis=1).reshape((-1,1))
    return exp / norms

def net(X, W):
    y_linear = np.dot(X, W)
    yhat = softmax(y_linear)
    return yhat

def cross_entropy(w, y=train_labels[:10000], ims=train_images[:10000]):
    yhat = net(ims, w.T)
    return - np.sum(y * np.log(yhat+1e-6))

def log_likelihood(w, ims=train_images[:10000], labs=train_labels[:, :10000]):
    l = 0
    pred = net(ims, np.transpose(w))


    argmax = np.max(pred, axis=0)
    return np.sum(argmax - logsumexp(pred, axis=0))/len(ims)




w = np.zeros((10, 784))




def optimize(num_iter, lrate, ims=train_images[:10000], labs=train_labels[:, :10000]):
    w = np.zeros((10, 784))

    for i in range(num_iter):
        grad = ag.grad(cross_entropy)
        params_grad = grad(w)
        print('done grad')
        print(i, "loss:", cross_entropy(w))
        w -= lrate * params_grad
        print(accuracy(w))
        print(log_likelihood(w))
    return w

def accuracy(w, ims=train_images[:10000], labels=train_labels[:10000]):
    pred = net(ims, np.transpose(w))

    acc = 0
    for i in range(len(ims)):
        if list(pred[i]).index(max(pred[i])) == np.where(labels[i] == 1)[0][0]:
            acc += 1
    return acc / len(ims)


print(log_likelihood(w))
w = optimize(500, 0.025)
save_images(w, 'logreg')
print(accuracy(w, test_images, test_labels))
print(accuracy(w))
print(log_likelihood(w, ims=test_images))
print(log_likelihood(w))


print(w)



